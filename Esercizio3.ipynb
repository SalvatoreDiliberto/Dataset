{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.python import BranchPythonOperator \n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import requests \n",
    "import time \n",
    "import json \n",
    "\n",
    "\n",
    "def get_data(**kwargs):\n",
    "    tickers = kwargs[\"tickers\"]\n",
    "    api_key = kwargs[\"api_key\"]\n",
    "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={tickers}&apikey={api_key}'\n",
    "    r = requests.get(url)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        path = \"C:/Users/salvo/OneDrive/Desktop/airflow/DATA_CENTER/DATA_LAKE\"\n",
    "        with open(path + \"stock_market_rawdata\" +tickers+'_'+ str(time.time()), \"w\") as f:\n",
    "            json.dump(data, f) \n",
    "    except:\n",
    "        print(\"Error\")\n",
    "\n",
    "#crontab notation  https://crontab.guru/#30_11_18_10_3\n",
    "\n",
    "default_dag_args = {\n",
    "    \"start_date\": datetime(2023,9,1),\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retryes\": 1,\n",
    "    \"retray_delay\": timedelta(minutes=5),\n",
    "    \"project_id\": 1\n",
    "}\n",
    "\n",
    "with DAG(\"market_data_alphavantage_dag\", schedule_interval = '@daily', catchup=False, default_args = default_dag_args) as dag_python:\n",
    "\n",
    "    task_0 = PythonOperator(task_id = \"get_market_data\", python_callable = get_data, op_kwargs = {'tickers' : \"IBM\"})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
